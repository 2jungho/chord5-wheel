# 멀티모달(이미지 업로드) 기능 제약사항 및 고려사항

## 1. 현재 앱(v1.1.5)의 구현 상태
*   **미구현**: 현재 AI 채팅 시스템(`ChatState`, `AIService`)은 **텍스트(Text)**와 **음성(STT, 텍스트 반환)** 입력만을 처리하도록 설계되어 있습니다.
*   **UI 부재**: 이미지 선택/업로드 버튼 및 미리보기 UI가 존재하지 않습니다.
*   **API 연동 부재**: Gemini/OpenAI API 호출 시 이미지 바이너리 데이터를 전송하는 로직이 포함되어 있지 않습니다.

## 2. 기술적 제약사항 (구현 시 고려사항)

### A. 모델 및 비용 (Model & Cost)
*   **필수 모델 등급**: 이미지를 인식하기 위해서는 멀티모달(Vision) 기능을 지원하는 모델을 사용해야 합니다.
    *   **Google**: Gemini 1.5 Flash, 1.5 Pro, 2.0 Flash 등 (현재 주력 모델들은 대부분 지원).
    *   **OpenAI**: GPT-4o, GPT-4 Turbo with Vision (GPT-3.5나 구형 모델은 지원 불가).
*   **비용 증가**: 이미지는 텍스트 대비 토큰 소모량이 월등히 높습니다. (예: 고해상도 이미지 1장당 수백~수천 토큰 소요 가능). API 비용 관리 및 사용자 할당량(Quota) 이슈가 발생할 수 있습니다.

### B. 인식 정확도의 한계 (Accuracy Limitations)
*   **악보(Score/Staff)**:
    *   전통적인 오선보 인식(OMR)은 매우 복잡한 영역입니다. LLM은 악보의 전반적인 키(Key), 박자, 코드 기호 등은 잘 읽을 수 있으나, **개별 음표의 정확한 멜로디 라인이나 리듬**을 완벽하게 텍스트나 MIDI로 변환하는 데에는 한계가 있습니다.
    *   *예: "이 악보의 조표가 뭐야?" (O), "이 악보 그대로 연주해줘" (X 또는 부정확)*
*   **타브 악보(TAB)**:
    *   숫자로 된 타브 악보는 텍스트 OMR과 유사하여 상대적으로 인식이 잘 될 수 있으나, 손글씨나 해상도가 낮은 이미지는 오류 가능성이 높습니다.
*   **기타 지판/코드 사진**:
    *   손 모양이나 지판 위의 손가락 위치를 보고 코드를 식별하는 것은 꽤 정확할 수 있으나, 조명, 각도, 손가락 가림(Occlusion) 여부에 따라 인식률이 달라집니다.

### C. 성능 및 UX (Performance)
*   **지연 시간(Latency)**: 이미지를 업로드하고 분석하는 데 텍스트보다 긴 시간(수 초 이상)이 소요됩니다. 로딩 인디케이터 등 UX 보완이 필수적입니다.
*   **데이터 전송**: 모바일 환경에서 고해상도 이미지 전송 시 데이터 사용량 및 업로드 시간이 문제될 수 있습니다. (클라이언트 측 리사이징/압축 필수)

## 3. 구현 로드맵 (제안)
기능 도입 시 다음과 같은 절차가 필요합니다.

1.  **패키지 추가**: `image_picker` (갤러리/카메라 접근).
2.  **UI 수정**: 채팅 입력창에 '이미지 첨부' 버튼 및 '미리보기' 영역 추가.
3.  **로직 확장**:
    *   `message` 모델: 텍스트 외에 `imagePath` 또는 `bytes` 필드 추가.
    *   `AIService` / `GeminiProvider`: `Content.text` 대신 `Content.multi` 형태로 요청 구조 변경.
4.  **프롬프트 최적화**: "이 이미지는 악보/타브/지판 사진이다. 여기서 [사용자 질문]에 대해 분석해라"와 같은 시스템 프롬프트 튜닝.
